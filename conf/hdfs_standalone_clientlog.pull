job.name=ClientLogKafka2HDFS
job.group=GobblinData4SAHDFS
job.description=Periodly pull data from Kafka from prod to HDFS, with avro format
job.lock.enable=false
job.schedule=0 30 0  * * ?

kafka.brokers=log2:9092
topic.blacklist=nginx
source.class=xiaomei.gobblin.core.source.extractor.kafka.KafkaGzipSource
extract.namespace=gobblin.extract.kafka

writer.builder.class=gobblin.writer.SimpleDataWriterBuilder
simple.writer.delimiter=\n
simple.writer.prepend.size=false
writer.file.path.type=tablename
writer.destination.type=HDFS
writer.output.format=csv

data.publisher.type=gobblin.publisher.BaseDataPublisher

mr.job.max.mappers=1

writer.staging.dir=${env:GOBBLIN_WORK_DIR}/client_log/task-staging
writer.output.dir=${env:GOBBLIN_WORK_DIR}/client_log/task-output

# Data publisher related configuration properties
data.publisher.final.dir=${env:GOBBLIN_WORK_DIR}/client_log/job-output

# Directory where job configuration files are stored
jobconf.dir=${env:GOBBLIN_JOB_CONFIG_DIR}

# Directory where job/task state files are stored
state.store.dir=${env:GOBBLIN_WORK_DIR}/client_log/state-store

# Directory where error files from the quality checkers are stored
qualitychecker.row.err.file=${env:GOBBLIN_WORK_DIR}/client_log/err

# Directory where job locks are stored
job.lock.dir=${env:GOBBLIN_WORK_DIR}/client_log/locks

# Directory where metrics log files are stored
metrics.log.dir=${env:GOBBLIN_WORK_DIR}/client_log/metrics

metrics.reporting.file.enabled=true
metrics.reporting.file.suffix=txt
bootstrap.with.offset=earliest
